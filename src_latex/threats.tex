\section{Threats to validity}\label{threats}

In this section, we discuss the threats to validity of our study following common guidelines for empirical studies~\cite{robert2002case}.

\textbf{Construct validity threats} concern the relation between theory and observation. In our study, threats to the construct validity are mainly due to measurement errors. %Our metrics in our survival analysis might not reflect all characteristics related to defects \Foutse{do you mean code smells or faults?} and we could include more metrics specifically related to source code or even the review process of fault fixes. \Foutse{you should explain why your results are still valid regardless of this limitation!} %We leave the selection of more metrics as a future work.
The number of previous faults in each source code file was calculated by identifying the files that were committed in a fault fixing revision. This technique is not without flaws. We identified fault fixing commits by mining the logs searching for certain keywords (\ie{} ``bug",``fix"',``defect" and ``patch") as explained in Section~\ref{extraction}. Following this approach, we are not able to detect fault fixing revisions if the committer either misspelled the keywords or failed to include any commit message. Nevertheless, this heuristic was successfully used in multiple previous studies in software engineering~\cite{jaafar2013mining,shihab2013studying}. The SZZ heuristic used to identify fault-inducing commits is not 100\% accurate. However, it has been successfully used in multiple previous studies from the literature, with satisfying results. In our implementation, we remove all fault-inducing commit candidates that only changed blank or comment lines. When analyzing the \emph{smelliness} of files that experienced fault-inducing changes, we only tracked the presence of the smell in the file as a whole. Hence, the smell contained in the file may not have been involved in the changed lines that induced the fault. %we assume the a smell didn't get touched if it is removed from a section of a file and another added to another section of the same file in the same commit.}

\textbf{Internal validity threats} concern our selection of systems and tools. % other possible explanations for some of our observations. We used SZZ algorithm to find original commits that induced bugs later in the project. The SZZ algorithm is a an heuristic approach and might not contain all 100\% true positive original commits.
The metric extraction tool used in this paper is based on the AST provided by ESLint. The results of the study are therefore dependent on the accuracy of ESLint. However, we are rather assured that this tool functions properly as it is being used widely by big companies. \eg{} Facebook, Paypal, Airbnb.
%The threshold selected to find smelly codes is by top 10\%  ... The different numbers for threshold might change the category of files as smelly or not smelly. Hence, different strategies to define a threshold may lead different results at the end.
We chose a logarithmic link function for some of our covariates in the survival analysis. It is possible that a different link function would be a better choice for these covariates. However, the non-proportionality test implies that the models were a good fit for the data. %We leave the selection of different link functions for each covariate as future work.
Also, we do not claim causation in this work, we simply report observations and correlations and tries to explain these findings.

\textbf{Threats to conclusion validity} address the relationship between the treatment and the outcome. We are careful to acknowledge the assumptions of each statistical test.

\textbf{Threats to external validity} concern the possibility to generalize our results. In this paper, we have studied {\color{blue}fifteen} large JavaScript projects. We have also limited our study to open-source projects. Still, these projects represent different domains and various project sizes. Table~\ref{studiedsystems} shows a summary of the studied systems, their domain and their size. Nevertheless, further validation on a larger set of JavaScript systems, considering more types of code smells is desirable. %done to further validate our results. %\New{We have limited this study to a list of 12 types of code smells; however, the list includes the most common bad practices in JavaScript. Future works should also consider other types of code smells.}

\textbf{Threats to reliability validity} concern the possibly of replicating our study. In this paper, we provide all the details needed to replicate our study. All our {\color{blue}fifteen} subject systems are publicly available for study. The data and scripts used in this study is also publicly available on Github\footnote{https://github.com/DavidJohannesWall/smells\_project}.
%Therefore, we can not generalize 100\% our results to other JavaScript projects particularly closed-source ones. Even though our study includes a small subset of available JavaScript software, we attempted to mitigate this issues by selecting a diverse number of projects.

{\color{blue}
\textbf{Threats to internal genealogy construction} is about our way to get the smells genealogy of the studied smells, more specifically the recognition of the smells over time and commits. Indeed, we set a similarity threshold of 70\%, meaning that if two smells of the same type have a similarity greater than 70\%, there are likely the same. Obviously, this threshold is not perfect and can associate two different smells together, or dissociate two smells, which are in reality the same. However, we changed it in order to see if some significant differences would appear, but no relevant difference was revealed.
	
}